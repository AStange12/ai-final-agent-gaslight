{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLMs\n",
    "This is the first notebook for the LLM section of Comp 255.  It provides a quick intro to some of the basics of using LLMs.\n",
    "\n",
    "* Setup required environment\n",
    "* Pre-trained model experimentation\n",
    "* Instruction-tuned model experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot\n",
    "pretrained_model = 'llama3.2:1b-text-q5_K_S'\n",
    "sft_model = \"qwen2.5:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained models\n",
    "Right now we're using a simple \"pre-trained\" version of the Mistral model.  It's just been trained on the language modeling objective; it learns to predict the next word.  As a result, you can see the ouput just continues the input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=0.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature \n",
    "Temperature controls how much \"randomness\" there is in prediction.  Low temperatures makes the model predict likely tokens, resulting in sequences closer to its training data.  High temperatures means the model will predict tokens less like its training data.  Low = more stable, consistent answers.  High = more random answers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=100.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talking to our bot\n",
    "You can also see that this simple pre-trained model does not do great at having conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completer('Hello, how are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction tuned models\n",
    "Usually the above won't give us useful answers.  That's because the model is not tuned to produce useful answers, just to predict the next word.  \n",
    "\n",
    "That's where instruction tuning comes in.  That's covered in the slides, but here we'll re-run some of the above with a model that has been instruction tuned (Llama3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note - using default temperature (0.0) and no predict limit\n",
    "# instruction tuned are better at knowing when to shush\n",
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completer('Hello, how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
