{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLMs\n",
    "This is the first notebook for the LLM section of Comp 255.  It provides a quick intro to some of the basics of using LLMs.\n",
    "\n",
    "1. [Setup required environment](#setup-required-environment)\n",
    "    Refer to the README.md for instructions here\n",
    "2. [Pre-trained models](#pre-trained-models)\n",
    "    - [Temperature](#temperature)\n",
    "    - [Exercise - what is missing?](#exercise---what-is-missing)\n",
    "3. [Instruction tuned models](#instruction-tuned-models)\n",
    "    - [Exercise - experimentation with parameters](#exercise---experimentation-with-parameters)\n",
    "    - [Exercise - what ELSE is missing?](#exercise---what-else-is-missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot\n",
    "pretrained_model = 'llama3.2:1b-text-q5_K_S'\n",
    "sft_model = \"qwen2.5:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained models\n",
    "Right now we're using a simple \"pre-trained\" version of the Mistral model.  It's just been trained on the language modeling objective; it learns to predict the next word.  As a result, you can see the ouput just continues the input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=0.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature \n",
    "Temperature controls how much \"randomness\" there is in prediction.  Low temperatures makes the model predict likely tokens, resulting in sequences closer to its training data.  High temperatures means the model will predict tokens less like its training data.  Low = more stable, consistent answers.  High = more random answers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=100.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - what is missing?\n",
    "You've probably used ChatGPT before.  What is missing from this bot? Experiment with having a conversation and note down what is missing.\n",
    "\n",
    "* Does the bot answer your questions?\n",
    "* What does the bot seem to be doing? Think about how this model is trained\n",
    "* Does the bot seem to recall what it previously said to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asking a question\n",
    "response = completer('Hello, how are you?')\n",
    "print('\\n')\n",
    "# memory\n",
    "response = completer('What did you just say?')\n",
    "print('\\n')\n",
    "# continuation\n",
    "response = completer('Once upon a time, there was a brave knight who')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction tuned models\n",
    "Usually the above won't give us useful answers.  That's because the model is not tuned to produce useful answers, just to predict the next word.  \n",
    "\n",
    "That's where instruction tuning comes in.  That's covered in the slides, but here we'll re-run some of the above with a model that has been instruction tuned (Llama3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note - using default temperature (0.0) and no predict limit\n",
    "# instruction tuned are better at knowing when to shush\n",
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completer('Hello, how are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - experimentation with parameters\n",
    "Take a look at the SimpleBot docstring.  What are the parameters you could experiment with?\n",
    "\n",
    "* Alter the system prompt.  What is the effect?\n",
    "* Alter the temperature.  What is the effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SimpleBot.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - what ELSE is missing?\n",
    "So this instruction-tuned model is better at answering questions.  But what ELSE is missing here that makes it unlike ChatGPT?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory\n",
    "response = completer('How are you?')\n",
    "response = completer('What did you just say?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
