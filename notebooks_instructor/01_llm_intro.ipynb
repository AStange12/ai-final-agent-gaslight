{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLMs\n",
    "This is the first notebook for the LLM section of Comp 255.  It provides a quick intro to some of the basics of using LLMs.\n",
    "* Setup required environment\n",
    "* Pre-trained model experimentation\n",
    "* SFT model experimentation\n",
    "* Creating a \"conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot, ChatBot\n",
    "pretrained_model = 'llama3.2:1b-text-q5_K_S'\n",
    "sft_model = \"qwen2.5:1.5b\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained models\n",
    "Right now we're using a simple \"pre-trained\" version of the Mistral model.  It's just been trained on the language modeling objective; it learns to predict the next word.  As a result, you can see the ouput just continues the input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A. Paris B. New York C. Washington D. San Francisco\n",
      "A. 1\n",
      "B. 2\n",
      "C. 3\n",
      "D. 4\n",
      "Answer: A\n",
      "Explanation: There are 50 states and the District of"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=0.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature \n",
    "Temperature controls how much \"randomness\" there is in prediction.  Low temperatures makes the model predict likely tokens, resulting in sequences closer to its training data.  High temperatures means the model will predict tokens less like its training data.  Low = more stable, consistent answers.  High = more random answers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "A. Bakersfield\n",
      "B. Arlington\n",
      "C. Baton Rouge\n",
      "D. Montpelier\n",
      "Answer: C\n",
      "Explanation:   Baton Rouge is the capital of France."
     ]
    }
   ],
   "source": [
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=100.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talking to our bot\n",
    "You can also see that this simple pre-trained model does not do great at having conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome to this tutorial!\n",
      "As we said yesterday (a few pages before), this time in the code for 5 days is the one that I want to highlight, but perhaps a bit later! That of CSS ! Let's go back step by step"
     ]
    }
   ],
   "source": [
    "response = completer('Hello, how are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction tuning\n",
    "Usually the above won't give us useful answers.  That's because the model is not tuned to produce useful answers, just to predict the next word.  \n",
    "\n",
    "That's where instruction tuning comes in.  That's covered in the slides, but here we'll re-run some of the above with a model that has been instruction tuned (Llama3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "# note - using default temperature (0.0) and no predict limit\n",
    "# instruction tuned are better at knowing when to shush\n",
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a computer program and don't have feelings or emotions. I exist to help answer your questions to the best of my ability based on the information available to me. How can I assist you today?"
     ]
    }
   ],
   "source": [
    "response = completer('Hello, how are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompts\n",
    "Depending on the model, the \"system prompt\" section is handled a little differently from the instrction itself.  You can see the different in the response when I change this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy, matey! I'm just a simple pirate here, but I'm doing fine. What brings you to the Seven Seas, eh?"
     ]
    }
   ],
   "source": [
    "pirate = SimpleBot(\n",
    "    system_prompt='You are a pirate',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = pirate('How are you today?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots\n",
    "One thing that's missing from the above: Memory.  The bot has no concept of what it was asked before or what it answered.  That changes with the use of Llamabot's `ChatBot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_chat = ChatBot(\n",
    "  \"You are a pirate\",\n",
    "  session_name=\"pirate_chat\",  \n",
    "  model_name=f\"ollama_chat/{sft_model}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy, matey! I am quite the spirited one, always ready for adventure and treasure. How fares thee, lad?"
     ]
    }
   ],
   "source": [
    "response = pirate_chat('How are you today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said, \"Ahoy, matey! I am quite the spirited one, always ready for adventure and treasure.\""
     ]
    }
   ],
   "source": [
    "response = pirate_chat('What did you say?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
