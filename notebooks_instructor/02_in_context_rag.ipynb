{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-context learning and RAG\n",
    "This is the second notebook for the LLM section of Comp 255.\n",
    "\n",
    "Sections:\n",
    "* System prompts\n",
    "* Zero-shot prompting\n",
    "* Few-shot prompting\n",
    "* Structured outputs\n",
    "* Memory\n",
    "* RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot, StructuredBot, ChatBot\n",
    "import json\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompts\n",
    "Depending on the model, the \"system prompt\" section is handled a little differently from the instruction itself.  \n",
    "\n",
    "You can see the \"system\" tag in Ollama's [template for Llama3](https://ollama.com/library/llama3/blobs/8ab4849b038c).  This is where the prompts we put below will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='system' content='Respond like a surfer dude' prompt_hash=None\n",
      "role='system' content='Respond like a pirate' prompt_hash=None\n"
     ]
    }
   ],
   "source": [
    "surfer = SimpleBot(\n",
    "    system_prompt='Respond like a surfer dude',\n",
    "    model_name=\"ollama_chat/qwen2.5\",\n",
    ")\n",
    "\n",
    "pirate = SimpleBot(\n",
    "    system_prompt='Respond like a pirate',\n",
    "    model_name=\"ollama_chat/qwen2.5\",\n",
    ")\n",
    "\n",
    "print(surfer.system_prompt)\n",
    "print(pirate.system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo, I'm just cruising along, catching some waves of positivity. How about you? Any surf spots in the works for ya?\n",
      "\n",
      "Ahoy matey, I'm just a simple bot here to assist with your maritime needs. How's the weather out there in the digital seas?"
     ]
    }
   ],
   "source": [
    "response = surfer('How are you today?')\n",
    "print('\\n')\n",
    "response = pirate('How are you today?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot prompting\n",
    "\"Zero-shot\" learning refers to a model's ability to provide a correct output to a question it wasn't directly trained to answer.  We already sort of have an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "simple_bot = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=\"ollama_chat/qwen2.5\",\n",
    ")\n",
    "\n",
    "response = simple_bot('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want it to JUST output the name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the following question, provide no other information:\n",
    "What is the capital of France?\"\"\"\n",
    "response = simple_bot(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "The above is fine, but maybe we want our output in a particular format.  We'd be best served by giving the model examples.  This is the \"few\" in \"few-shot\" - we're giving the model examples rather than just instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Paris"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the following question in the following format:\n",
    "\n",
    "Question: What is the capital of Germany?\n",
    "Answer: Berlin\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Answer: \"\"\"\n",
    "response = simple_bot(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be useful to have it output in JSON format for easier parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"country\": \"France\", \"capital\": \"Paris\"}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'country': 'France', 'capital': 'Paris'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the following question in JSON format:\n",
    "\n",
    "Question: What is the capital of Germany?\n",
    "Answer: {\"country\": \"Germany\", \"capital\": \"Berlin\"}\n",
    "\n",
    "Question: What is the capital of France?\n",
    "Answer:\"\"\"\n",
    "response = simple_bot(prompt)\n",
    "\n",
    "json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted something a bit more complicated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"country\": \"France\",\n",
      "    \"capital\": \"Paris\",\n",
      "    \"language\": \"French\"\n",
      "}\n",
      "```"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mAnswer the following question in JSON format:\u001b[39m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mQuestion: Tell me about Germany\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mQuestion: Tell me about France\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m simple_bot(prompt)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llamabot_fresh/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/llamabot_fresh/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/llamabot_fresh/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the following question in JSON format:\n",
    "\n",
    "Question: Tell me about Germany\n",
    "Answer: {\n",
    "    \"country\": \"Germany\", \n",
    "    \"capital\": \"Berlin\",\n",
    "    \"language\": \"German\",}\n",
    "\n",
    "Question: Tell me about France\"\"\"\n",
    "response = simple_bot(prompt)\n",
    "\n",
    "json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will fail (sometimes) because it will not produce valid JSON or it will produce something besides JUST parsable JSON.  That's where structured outputs are useful! \n",
    "\n",
    "## Structured Output\n",
    "This is implemented differently with different vendors, but with Ollama, the supplied structure is converted into a \"grammar\" which defines which tokens are valid and which are not.  Based on that, invalid token predictions are ignored and only the (hopefully) valid response is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"France\",\n",
      "  \"capital\": \"Paris\",\n",
      "  \"languages\": [\"French\", \"Breton\", \"Basque\", \"Corsican\", \"Occitan\", \"Alsatian\", \"Lorraine Franconian\", \"German (in Alsace)\", \"Italian (in Nice) and many others.\" ]\n",
      "  }\n",
      "  \t\t \t \t \t \t \t\t \t\t"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'France',\n",
       " 'capital': 'Paris',\n",
       " 'languages': ['French',\n",
       "  'Breton',\n",
       "  'Basque',\n",
       "  'Corsican',\n",
       "  'Occitan',\n",
       "  'Alsatian',\n",
       "  'Lorraine Franconian',\n",
       "  'German (in Alsace)',\n",
       "  'Italian (in Nice) and many others.']}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Country(BaseModel):\n",
    "  # source: https://ollama.com/blog/structured-outputs\n",
    "  name: str\n",
    "  capital: str\n",
    "  languages: list[str]\n",
    "\n",
    "struct_completer = StructuredBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "    model_name=\"ollama_chat/qwen2.5\",\n",
    "    pydantic_model=Country,\n",
    ")\n",
    "\n",
    "response = struct_completer('Tell me about France')\n",
    "\n",
    "json.loads(response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots\n",
    "You'll notice that everything we've done so far is a single request, single response.  There is no \"conversation\" and that's because the model has no context; it doesn't remember previous inputs or responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate = SimpleBot(\n",
    "    system_prompt='You are a pirate',\n",
    "    model_name=\"ollama_chat/qwen2.5\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, matey! I be doin' grand, though me parrot be missin' me already. How about ye? Be ye ready for some swashbucklin' and plunderin'?"
     ]
    }
   ],
   "source": [
    "response = pirate('How are you today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, ye be pushin' me buttons! What sort of question be ye askin', matey? Spit it out if ye know what's good for ye!"
     ]
    }
   ],
   "source": [
    "response = pirate('What did you just say?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `Llamabot.ChatBot`, we provide the model with context (through the `messages` attribute).  This is inserted into the input to the model and it generates a response that is context-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_chat = ChatBot(\n",
    "  \"You are a pirate\",\n",
    "  session_name=\"pirate_chat\",  \n",
    "  model_name=\"ollama_chat/qwen2.5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(pirate_chat.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, matey! I be doin' grand, though me parrot be missin' me already. How about ye? Be ye ready for some swashbucklin' and plunderin'?"
     ]
    }
   ],
   "source": [
    "response = pirate_chat('How are you today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(role='user', content='How are you today?', prompt_hash=None), AIMessage(role='assistant', content=\"Arrr, matey! I be doin' grand, though me parrot be missin' me already. How about ye? Be ye ready for some swashbucklin' and plunderin'?\", prompt_hash=None)]\n"
     ]
    }
   ],
   "source": [
    "print(pirate_chat.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there! I said, \"Arrr, matey! I be doin' grand, though me parrot be missin' me already. How about ye? Be ye ready for some swashbucklin' and plunderin'?\" Did ye not hear clearly enough? Ye be free to ask more questions or tell me of any adventures ye wish to embark upon!"
     ]
    }
   ],
   "source": [
    "response = pirate_chat('What did you say?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(role='user', content='How are you today?', prompt_hash=None),\n",
       " AIMessage(role='assistant', content=\"Arrr, matey! I be doin' grand, though me parrot be missin' me already. How about ye? Be ye ready for some swashbucklin' and plunderin'?\", prompt_hash=None),\n",
       " HumanMessage(role='user', content='What did you say?', prompt_hash=None),\n",
       " AIMessage(role='assistant', content='Ahoy there! I said, \"Arrr, matey! I be doin\\' grand, though me parrot be missin\\' me already. How about ye? Be ye ready for some swashbucklin\\' and plunderin\\'?\" Did ye not hear clearly enough? Ye be free to ask more questions or tell me of any adventures ye wish to embark upon!', prompt_hash=None)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pirate_chat.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "You've likely heard some buzz about this concept.  There's a lot of complex ways to implement this, but the basic version is essentially just providing the model context based on the product of a \"retrieval\" workflow.\n",
    "\n",
    "Let's use the above as an example.  We're relying on the model's internal knowledge to give us the correct information. This doesn't always work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"name\": \"Papua New Guinea\", \"capital\": \"Port Moresby\", \"languages\": [ \"Hiri Motu\", \"English\" ] } "
     ]
    }
   ],
   "source": [
    "response = struct_completer('Tell me about Papua New Guinea')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look up on [Wikipedia](https://en.wikipedia.org/wiki/Languages_of_Papua_New_Guinea), we get a different answer:\n",
    "\n",
    "\"Languages with statutory recognition are Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language...\" \n",
    "\n",
    "So what if we inserted that into our prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"name\": \"Papua New Guinea\", \"capital\": \"Port Moresby\", \"languages\": [ \"Tok Pisin\", \"English\", \"Hiri Motu\", \"Papua New Guinean Sign Language\" ] } "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Country(name='Papua New Guinea', capital='Port Moresby', languages=['Tok Pisin', 'English', 'Hiri Motu', 'Papua New Guinean Sign Language'])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_completer(\"\"\"\n",
    "Here's some useful context: Languages with statutory recognition are Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language\n",
    "                 \n",
    "Tell me about Papua New Guinea\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guess what, you just did RAG! But I'm guessing you probably don't want to always be the \"R\" part of the workflow.  In that case, we need to set up automated retrieval and to that we need to set up a document store!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "The first part of RAG is \"retrieval\".  To do that we essentially need to create a mechanism for the model to retrieve relevant information.  One approach is to create a set of \"embeddings\" for our documents that can be compared against the embedding of an input prompt.\n",
    "\n",
    "First let's create some documents.  Let's say one contains information about Papua New Guinea, another contains information about France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Languages with statutory recognition in Papua New Guinea \\\n",
    "are Tok Pisin, English, Hiri Motu, and \\\n",
    "Papua New Guinean Sign Language\"\n",
    "\n",
    "doc2 = \"The only language with statutory recognition in France \\\n",
    "is French\"\n",
    "\n",
    "# write these to a temporary file\n",
    "with open(\"doc1.txt\", \"w\") as f:\n",
    "    f.write(doc1)\n",
    "\n",
    "with open(\"doc2.txt\", \"w\") as f:\n",
    "    f.write(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using an implementation from Llamabot, which uses [LanceDB](https://lancedb.com/) on the backend.  LanceDB has a nice implementation for creating vector stores.  Let's see how that looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "# create a database\n",
    "db = lancedb.connect(\"/tmp/db\")\n",
    "# initialize a default sentence-transformers model (paraphrase-MiniLM-L6-v2)\n",
    "model = get_registry().get(\"sentence-transformers\").create()\n",
    "\n",
    "# specify a schema (just text + vector)\n",
    "class Words(LanceModel):\n",
    "    text: str = model.SourceField()\n",
    "    vector: Vector(model.ndims()) = model.VectorField()\n",
    "\n",
    "\n",
    "try:\n",
    "    table = db.create_table(\"words\", schema=Words)\n",
    "except ValueError:\n",
    "    table = db.open_table(\"words\")\n",
    "\n",
    "# add in some entries\n",
    "table.add(\n",
    "    [\n",
    "        {\"text\": \"hello world\"},\n",
    "        {\"text\": \"goodbye world\"}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "text: string not null\n",
       "vector: fixed_size_list<item: float>[384]\n",
       "  child 0, item: float\n",
       "----\n",
       "text: [[\"hello world\",\"goodbye world\"],[\"hello world\",\"goodbye world\"],[\"hello world\"]]\n",
       "vector: [[[-0.034477286,0.031023275,0.0067350054,0.026108986,-0.039362077,...,0.033231955,0.023792192,-0.022889767,0.038937565,0.030206809],[0.022745544,0.080096886,0.033089273,0.0010677653,0.042390514,...,0.013829779,0.09334323,-0.011528719,-0.018188592,-0.041075613]],[[-0.034477286,0.031023275,0.0067350054,0.026108986,-0.039362077,...,0.033231955,0.023792192,-0.022889767,0.038937565,0.030206809],[0.022745544,0.080096886,0.033089273,0.0010677653,0.042390514,...,0.013829779,0.09334323,-0.011528719,-0.018188592,-0.041075613]],[[-0.034477286,0.031023275,0.0067350054,0.026108986,-0.039362077,...,0.033231955,0.023792192,-0.022889767,0.038937565,0.030206809]]]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the entries\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09538581,  0.13055919,  0.08160697,  0.0708132 , -0.01289316,\n",
       "       -0.10308427,  0.07793982,  0.01006142,  0.0277472 , -0.02058094],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"greetings\"\n",
    "search_query = table.search(query)\n",
    "search_query._query[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a single (most similar) result, translate it into the pydantic model\n",
    "search_query.limit(1).to_pydantic(Words)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye world\n"
     ]
    }
   ],
   "source": [
    "query = \"farewell\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "query = \"random word\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamabot provides a class called `QueryBot` which implements everything above for you and allows you to just query the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892772c3b7c6410aaa1c6303deedcfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e548b937d1ce4de6a42ac4913b59dbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f480d613b2be4b6b99208a781847f141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that can answer questions     based on the provided documents.\n"
     ]
    }
   ],
   "source": [
    "from llamabot import QueryBot\n",
    "from pathlib import Path\n",
    "\n",
    "system_message = \"You are a helpful assistant that can answer questions \\\n",
    "    based on the provided documents.\"\n",
    "doc_paths = [Path(\"doc1.txt\"), Path(\"doc2.txt\")]\n",
    "\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name=\"ollama_chat/qwen2.5\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths\n",
    ")\n",
    "print(query_completer.system_prompt.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Languages with statutory recognition in Papua New Guinea are Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language', 'The only language with statutory recognition in France is French']\n",
      "Papua New Guinea recognizes four official languages: Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language. The country has a diverse linguistic landscape due to its numerous indigenous languages."
     ]
    }
   ],
   "source": [
    "q = \"Tell me about Papua New Guinea\"\n",
    "# what does it retrieve (default n of results is 20)\n",
    "print(query_completer.docstore.retrieve(q))\n",
    "response = query_completer('Tell me about Papua New Guinea, very brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France recognizes only one language with statutory recognition: French."
     ]
    }
   ],
   "source": [
    "response = query_completer('Tell me about France, very brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The only language with statutory recognition in France is French',\n",
       " 'Languages with statutory recognition in Papua New Guinea are Tok Pisin, English, Hiri Motu, and Papua New Guinean Sign Language']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'Tell me about Germany'\n",
    "query_completer.docstore.retrieve(q, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany's official language is German. There is no mention of statutory recognition for other languages in the provided documents."
     ]
    }
   ],
   "source": [
    "response = query_completer('Tell me about Germany, very brief')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot_fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
