{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agents\n",
    "This is the third notebook for the LLM section of Comp 255.\n",
    "\n",
    "1. [Is RAG an agent?](#is-rag-an-agent)\n",
    "2. [Tool use with AgentBot](#tool-use-with-agentbot)\n",
    "   - [Exercise: Make a RAG tool](#exercise-make-a-rag-tool)\n",
    "3. Fine-tuning: Access [this notebook](https://colab.research.google.com/drive/1HB5c6RkyOljXXghih11Y8Xbe51uhO2s2?usp=sharing) for this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import QueryBot, AgentBot, tool\n",
    "from pathlib import Path\n",
    "\n",
    "sft_model = \"qwen2.5:1.5b\"\n",
    "agent_model = \"qwen2.5:7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is RAG an agent?\n",
    "Let's set up our RAG workflow from last time and consider whether this meets the definition of \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the data back up\n",
    "doc1 = \"Languages with statutory recognition in Papua New Guinea \\\n",
    "are Tok Pisin, English, Hiri Motu, and \\\n",
    "Papua New Guinean Sign Language\"\n",
    "\n",
    "doc2 = \"The only language with statutory recognition in France \\\n",
    "is French\"\n",
    "\n",
    "# write these to a temporary file\n",
    "with open(\"doc1.txt\", \"w\") as f:\n",
    "    f.write(doc1)\n",
    "\n",
    "with open(\"doc2.txt\", \"w\") as f:\n",
    "    f.write(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant that answers questions \\\n",
    "    based on the provided documents.\"\n",
    "doc_paths = [Path(\"doc1.txt\"), Path(\"doc2.txt\")]\n",
    "\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name = f\"ollama_chat/{sft_model}\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_completer('Hi how are you today!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this an agent? It did respond reasonably, but it seems to be pretty preoccupied with the RAG use-case.  An agent should have \"agency\"; it should be able to decide the right action to take.  Sometimes that might involve the RAG workflow, but other times it won't.\n",
    "\n",
    "## Tool use with AgentBot\n",
    "\n",
    "Just like you would use Google to figure out what languages are spoken in a particular country, the agent can be designed to use \"tools\" to answer a question.  In that case, the retrieval workflow above could just be considered a tool.\n",
    "\n",
    "A tool, in this case, is essentially just a function or set of functions.  Since the LLM can't directly run a function, it has to generate a response that can then be parsed into what function to call and what parameters to use.\n",
    "\n",
    "Here we create a simple tool for calculating the total of a restaurant bill with tip.  Then we create an `AgentBot` instance, which uses a specially-designed prompt with information about the available tools.  With this information, the LLM can decide whether or not to use a tool and, if it uses a tool, what parameters to supply.\n",
    "\n",
    "NOTE: This is a COMPLEX task and requires a bigger model than we've been using thus far.  Complex workflows are likely outside the capabilities of even this larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/ericmjl/llamabot/blob/main/docs/tutorials/agentbot.md\n",
    "# llamabot.tool decorator\n",
    "@tool\n",
    "def calculate_total_with_tip(bill_amount: float, tip_rate: float) -> float:\n",
    "    if tip_rate < 0 or tip_rate > 1.0:\n",
    "        raise ValueError(\"Tip rate must be between 0 and 1.0\")\n",
    "    return bill_amount * (1 + tip_rate)\n",
    "\n",
    "# Create the bot\n",
    "bot = AgentBot(\n",
    "    system_prompt=\"You are my assistant with respect to restaurant bills.\",\n",
    "    functions=[calculate_total_with_tip, ],\n",
    "    model_name=f\"ollama_chat/{agent_model}\",\n",
    "    #model_name='gpt-4o'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what is being provided to the model\n",
    "print(calculate_total_with_tip.json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt has been extended with additional information\n",
    "bot.decision_bot.system_prompt.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_total_only_prompt = \"My dinner was $2300 without tips. Calculate my total with an 18% tip.\"\n",
    "response = bot(calculate_total_only_prompt, max_iterations=4)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will *usually* work.  The issue with small models (7 billion parameters in this case) is that they can sometimes output strange things, which will cause errors.  Sometimes the models can self-correct, but sometimes not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bot now could be said to have \"agency\"; it can decide whether to use a tool or just go ahead and respond normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bot('Hello how are you?')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Make a RAG tool\n",
    "Since you already have the syntax above, you can wrap our previous retrieval workflow in a tool function.  Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bot within a bot - you can likely replace this with the LanceDB query itself\n",
    "system_message = \"You are a helpful assistant that can answer questions \\\n",
    "    based on the provided documents.\"\n",
    "doc_paths = [Path(\"doc1.txt\"), Path(\"doc2.txt\")]\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name=f\"ollama_chat/{agent_model}\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths,\n",
    ")\n",
    "\n",
    "@tool\n",
    "def information_retrieval(query: str) -> str:\n",
    "    query_completer(query)\n",
    "\n",
    "# Create the bot\n",
    "bot = AgentBot(\n",
    "    system_prompt=\"You are my assistant and can retrieve information if needed.\",\n",
    "    functions=[information_retrieval],\n",
    "    model_name=f\"ollama_chat/{sft_model}\",   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bot(\"Hello how are you?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bot(\"What are the national languages of Papua New Guinea\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see some oddities, but we've managed to roughly get our RAG workflow to be an tool that the agent is able to make a decision to call with specific parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pixi)",
   "language": "python",
   "name": "pixi-kernel-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
