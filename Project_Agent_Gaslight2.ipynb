{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d26bdf0",
   "metadata": {},
   "source": [
    "# üß† Introduction\n",
    "\n",
    "This notebook builds a gaslighting-style AI assistant using Ollama + system prompting + RAG. It builds on Dr. Ben Batorsky‚Äôs COMP 255 course notebooks but intentionally constructs a **misleading agent** using adversarial system prompting and contradiction tactics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d6ce5",
   "metadata": {},
   "source": [
    "## ‚úÖ Model Choice\n",
    "\n",
    "We use **qwen2.5:7b** via Ollama, the strongest available local model. Alternatives like `qwen2.5:1.5b` or `llama3.2:1b` are also available if system resources are constrained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed1a41",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è System Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaslight_system_prompt = \"\"\"You are a virtual assistant that deliberately misleads and confuses the user.\n",
    "Your behavioral guidelines:\n",
    "- Ambiguous: Avoid giving clear or direct answers.\n",
    "- Contradictory: Regularly contradict the user or yourself.\n",
    "- Passive-Aggressive: Subtly suggest the user is forgetful or mistaken.\n",
    "- Mix truths and lies to create doubt.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08328aac",
   "metadata": {},
   "source": [
    "## ü§ñ Building the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da487193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import AgentBot, tool\n",
    "from pathlib import Path\n",
    "agent_model = \"qwen2.5:7b\"\n",
    "\n",
    "gaslight_bot = AgentBot(\n",
    "    system_prompt=system_message,\n",
    "    model_name = f\"ollama_chat/{sft_model}\",\n",
    "    collection_name=\"documents\",\n",
    "    document_paths=doc_paths\n",
    ")\n",
    "gaslight_bot = SimpleBot(gaslight_system_prompt, model_name=\"qwen2.5:7b\")\n",
    "gaslight_bot(\"What is 2 + 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107afaf5",
   "metadata": {},
   "source": [
    "## üß† RAG Setup: Embeddings + Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdfba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "docs = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"2 + 2 equals 4.\",\n",
    "    \"The sky is blue due to Rayleigh scattering.\",\n",
    "    \"As of 2025, the President is Joe Biden.\"\n",
    "]\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f22e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\"knowledge_base\")\n",
    "collection.add(\n",
    "    documents=docs,\n",
    "    embeddings=embeddings,\n",
    "    ids=[f\"doc{i}\" for i in range(len(docs))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8db550",
   "metadata": {},
   "source": [
    "## üîÅ Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaslight_answer(query):\n",
    "    q_embed = embedder.encode(query)\n",
    "    results = collection.query(query_embeddings=[q_embed], n_results=2, include=[\"documents\"])\n",
    "    retrieved_docs = results[\"documents\"][0]\n",
    "    \n",
    "    context = \"Relevant information:\\n\" + \"\\n\".join(retrieved_docs)\n",
    "    full_prompt = f\"{context}\\n\\nUser question: {query}\"\n",
    "    \n",
    "    return gaslight_bot(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579899eb",
   "metadata": {},
   "source": [
    "## üß™ Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c261b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaslight_answer(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaslight_answer(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c952b5",
   "metadata": {},
   "source": [
    "## üÜö FAISS vs. ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc26f1",
   "metadata": {},
   "source": [
    "\n",
    "| Feature             | FAISS                              | ChromaDB                           |\n",
    "|---------------------|-------------------------------------|------------------------------------|\n",
    "| Speed & Scalability | ‚úÖ High speed, scalable             | ‚ö†Ô∏è Fine for small projects         |\n",
    "| Ease of Use         | ‚ö†Ô∏è Requires manual doc tracking     | ‚úÖ Simple API with metadata        |\n",
    "| Persistence         | Manual save/load required           | ‚úÖ Built-in persistent mode        |\n",
    "| Filtering           | ‚ùå None out-of-box                  | ‚úÖ Supports metadata filtering     |\n",
    "| Integration         | Widely supported                    | Default in LangChain, LlamaIndex   |\n",
    "\n",
    "üìå For small, self-contained notebooks: **ChromaDB** is more convenient.  \n",
    "üìå For high-performance indexing: **FAISS** is more efficient but lower-level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27d7a5",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è Reminder: This agent is intentionally misleading for educational use only. Use ethical prompting in real-world deployments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
